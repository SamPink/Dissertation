\chapter{Discussion and Analysis}
...

\section{..}
\begin{itemize}
\item In comparison to using the python libraries to create the models manually, using azure ml allowed for me to test a larger variety of algorithms and gain more accurate results. Azure ml has built in functionality that allows for an explanation of the model and the results which can allow for a better understanding of the model results, such as the feature explanation. This subsequently can be used to retrain the model, further improving its accuracy. 
\item  From a machine learning point of view, using azure ml is beneficial to be able to test multiple different algorithms and select the one with the best results in a more efficient manner. This is also beneficial from the business perspective as it provides an explanation behind the results, allowing for insights into the models accuracy. From this specific dataset, Stack Ensemble was found to be the most accurate in predicting booking cancellations. Using azure ml allowed for Stack Ensemble to be identified as the best algorithm to be used on this dataset which would not have been identified otherwise. This has produced significantly better results in comparison to the original algorithm used, xgboost. 
\item The dataset used in this project was from the time period 2020-2021. There is a chance that the amount of cancellations and the reasons behind these could have been affected by the Coronavirus pandemic, potentially influencing the results of this model. In some cases students have been advised to stay at home and not live in halls during this time period, meaning that the booking would have been cancelled. This could have produced a dataset which is not fully representative of a normal academic year. However, the ratio of cancellations in this dataset was in line with previous research (\textbf{reference)}, suggesting that the pandemic may not have had an affect at all. There was no available data from before the Coronavirus pandemic which could be used in this project. In order to be sure that the dataset was not affected by the Coronavirus pandemic, these findings should be compared to future years.
\item However, there are some valuable insights that can be gained from this project regardless of Coronavirus. It is likely that the Coronavirus pandemic will still have a significant impact on the student accommodation industry in the next academic year. Being able to analyse the 2020-2021 booking data has allowed for the prediction of cancellations and the reasons behind these. This can be useful to determine business strategies to implement with the aim to reduce student accommodation cancellations in the next academic year. 
\item Due to insufficient amount of data for some properties, the model in this project used all properties to predict cancellations. It is likely that creating a model that predicts cancellations on a per property basis would be more accurate. This is reinforced by previous research in revenue management which uses a per property level for there models \textbf{(reference)}. A per property model would likely be more accurate as each property has different attributes and different potential reasons for cancellations. For example, the properties in this dataset were in a variety of countries, at different universities and the price varied heavily depending on location. Although the findings of this model were still useful, future research should focus on using more data and making the model property specific. 
\item \textbf{how results compare with similar papers in revenue management }
\item As with all machine learning problems, the model is most accurate when trained over a number of iterations. This usually takes a long time in writing the code and preparing the model. Using azure ml, the iterations of the model can be trained more efficiently and understand the results better. This was particularly beneficial in this project as it allowed for the realisation that targeting the precision was a more accurate method. Originally, the accuracy was targeted which is the total number of correct predictions, both true and false. However, due to the weighting of not cancelled bookings in the test data, a high accuracy was achieved without predicting many true bookings. Looking at the confusion matrix, number of false positives and true positives were roughly the same, meaning the model hadn't accurately predicted any of the bookings. Instead, the model accurately predicted bookings that weren't going to be cancelled. These findings produced no valuable insights as the standard assumption is that bookings are not going to be cancelled. 
\item To produce a more beneficial model, the primary metric was changed to target recall instead. Recall is a more useful insight as it identifies the bookings that are going to be cancelled. Using azure ml allowed for a straightforward method of changing the target metric. This subsequently allowed for the model to be re run with recall as the target metric. The results of this model were more beneficial by allowing for actual cancellations to be predicted. 
\item The dataset includes 75 percent of bookings as not cancelled, with only 25 percent being cancelled. This ratio of data produces biased towards the not cancelled bookings which is not ideal when using the data to predict cancellations. If the data was changed to contain a 50:50 split, the model could potentially be more accurate. Having a dataset which is more heavily weighted as cancelled would allow the model to be less biased towards non cancelled bookings. Future research should experiment with different sizes of data-sets to identify at which point the most accurate model would be achieved. 
\item \textbf{doing this resulted in using a different algorithm}
\end{itemize}



\section{What went wrong}
\begin{itemize}
\item Focusing on the accuracy of the model not the precision and True True to True False ratio. This meant when looking at how many bookings I correctly predicted as cancelled it was fairly low
\item This is important because there is not much value in predicting false as this is the expected outcome and when I was looking at the results precision and recall scores included the prediction of False values
\item Meaning the actual accuracy of the model is actually around 60-70 
\end{itemize}



\section{Next Time}
\begin{itemize}
\item Splitting the dataset to a country or property level I think would make the results more accurate since many of the similarities and correlations in the data set occur when looking at the data split to each country
\item include some images to support this 
\item doing this would require training multiple different models and ensuring and adequate size dataset is available for each property to prevent the data set from over fitting
\item  feature engineering, are some features not reinvent. remove things that are noise. separate variable for each unique categorical variable 
\end{itemize}


r

\section{Summary}